# General-Optimization-Function
***
* This function can do all types of first order optimzers with three types of gradient descent <br>
* This notebook contain general implementaion for multivariable with adam with mini-batch <br>
* This implementaion can do all types of first order optimzers with small changing in parameters <br>
* After implementation, I tested it using different parameters and plot the change in losses 
